{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import time\n",
    "from L96_updated import *\n",
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_no_saving_of_xarray_burn_in(initial_X,initial_Y,mtu, data_file_path_to_save, data_with_y_file_path_to_save,forcing):\n",
    "    \n",
    "    \"\"\"\n",
    "    Burns in\n",
    "    \n",
    "    saves datasets with x and advection, and a separate one with x, advection and y.\n",
    "    \n",
    "    Save paths look like this: \"data/truth_run/training_dataset.npy\" and \"data/truth_run/training_dataset_with_y.npy\"\n",
    "    \n",
    "    Returns the last x and y values to init the next simulator. \n",
    "    \"\"\"\n",
    "    \n",
    "    l96_two= L96TwoLevel_updated(save_dt=save_time_step, X_init=initial_X, Y_init=initial_Y,K=k, J=J, h=1, F=forcing, c=10, b=10, dt=time_step)\n",
    "    l96_two.iterate(int(burn_in_mtu+mtu))\n",
    "    h2_xarray = l96_two.history\n",
    "    \n",
    "    x = np.ravel(h2_xarray.X)\n",
    "    x_subset = x.reshape(-1,k) #shape (timesteps, k)\n",
    "    advection = np.roll(x_subset, 1,axis=1) * (np.roll(x_subset, 2,axis=1) - np.roll(x_subset, -1,axis=1))\n",
    "    data = np.stack([x_subset,advection],axis=2)\n",
    "    y  = np.ravel(h2_xarray.Y).reshape(-1,k,J)\n",
    "    data_with_y = np.concatenate([data,y],axis=2)\n",
    "\n",
    "    del h2_xarray\n",
    "    \n",
    "    np.save(data_file_path_to_save,data[int(burn_in_mtu/save_time_step):])\n",
    "    np.save(data_with_y_file_path_to_save,data_with_y[int(burn_in_mtu/save_time_step):])\n",
    "\n",
    "    \n",
    "    initX_new = x_subset[-1,:]\n",
    "    initY_new = y[-1,:,:].reshape(k*J)\n",
    "    \n",
    "    return initX_new, initY_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_no_saving_of_xarray(initial_X,initial_Y,mtu, data_file_path_to_save, data_with_y_file_path_to_save,forcing):\n",
    "    \n",
    "    \"\"\"\n",
    "    No burn in\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    l96_two= L96TwoLevel_updated(save_dt=save_time_step, X_init=initial_X, Y_init=initial_Y,K=k, J=J, h=1, F=forcing, c=10, b=10, dt=time_step)\n",
    "    l96_two.iterate(mtu)\n",
    "    h2_xarray = l96_two.history\n",
    "    \n",
    "    x = np.ravel(h2_xarray.X)\n",
    "    x_subset = x.reshape(-1,k) #shape (timesteps, k)\n",
    "    advection = np.roll(x_subset, 1,axis=1) * (np.roll(x_subset, 2,axis=1) - np.roll(x_subset, -1,axis=1))\n",
    "    data = np.stack([x_subset,advection],axis=2)\n",
    "    y  = np.ravel(h2_xarray.Y).reshape(-1,k,J)\n",
    "    data_with_y = np.concatenate([data,y],axis=2)\n",
    "    \n",
    "    del h2_xarray\n",
    "    \n",
    "    np.save(data_file_path_to_save,data[1:])\n",
    "    np.save(data_with_y_file_path_to_save,data_with_y[1:])\n",
    "    \n",
    "    initX_new = x_subset[-1,:]\n",
    "    initY_new = y[-1,:,:].reshape(k*J)\n",
    "    \n",
    "    return initX_new, initY_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8\n",
    "J = 32\n",
    "burn_in_mtu = 2\n",
    "time_step = 0.001\n",
    "save_time_step = 0.005\n",
    "\n",
    "initX = np.zeros(shape=k)\n",
    "initY = np.zeros(shape=J*k)\n",
    "initX[0] = 1\n",
    "initY[0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One MTU is one 'model time unit' i.e. time_step = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example - 2000 MTU for F = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = create_datasets_no_saving_of_xarray_burn_in(initX,initY,2000,\"../data/truth_run/training_dataset.npy\",\"../data/truth_run/training_dataset_with_y.npy\",20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below can be run to split up the generation process if you want to create a big dataset but there are OOM issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data = np.load(\"../data/truth_run/training_dataset_with_y.npy\")\n",
    "\n",
    "initX_extra = initial_data[-1,:,0]\n",
    "initY_extra = initial_data[-1,:,2:].reshape(k*J)\n",
    "\n",
    "_, _ = create_datasets_no_saving_of_xarray(initX_extra,initY_extra,25000,\"../data/truth_run/extra.npy\",\"../data/truth_run/extra_with_y.npy\",20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = np.load(\"../data/truth_run/extra_with_y.npy\")\n",
    "\n",
    "initX_extra2 = extra[-1,:,0]\n",
    "initY_extra2 = extra[-1,:,2:].reshape(k*J)\n",
    "\n",
    "_, _ = create_datasets_no_saving_of_xarray(initX_extra2,initY_extra2,25000,\"../data/truth_run/extra2.npy\",\"../data/truth_run/extra2_with_y.npy\",20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a7e41bd27888a01a413951f5c72fb52d5ff134bd76168cac48078309413b6b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
